{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a53b642-9b81-40f4-9619-d4c303aacd73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::566765136679:role/service-role/AmazonSageMaker-ExecutionRole-20231121T121402\n",
      "sagemaker bucket: sagemaker-us-east-1-566765136679\n",
      "sagemaker session region: us-east-1\n",
      "data uri: s3://sagemaker-us-east-1-566765136679/whisper/data/ga-common-voice-processed\n"
     ]
    }
   ],
   "source": [
    "# Initialize sagemaker session and get the training data s3 uri\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import sagemaker.huggingface\n",
    "import os\n",
    "\n",
    "#BUCKET=\"[BUCKET_NAME]\" # please use your bucket name if you are not using the default bucket\n",
    "ROLE = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = sess.default_bucket()\n",
    "PREFIX = \"whisper/data/ga-common-voice-processed\"\n",
    "s3uri = os.path.join(\"s3://\", BUCKET, PREFIX)\n",
    "print(f\"sagemaker role arn: {ROLE}\")\n",
    "print(f\"sagemaker bucket: {BUCKET}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"data uri: {s3uri}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f73d376-00a3-4b8b-8868-6e2f69ea0033",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Some training parameters\n",
    "# For distributed training\n",
    "# distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "# instance_type = 'ml.p3.16xlarge'\n",
    "# training_batch_size  = 4\n",
    "# eval_batch_size = 2\n",
    "\n",
    "# For single instance training\n",
    "distribution = None\n",
    "instance_type = 'ml.p3.2xlarge'\n",
    "training_batch_size  = 16\n",
    "eval_batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c5b536-d310-465e-89cd-0ac47a41ad7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name:  whisper-ga-1700694190\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# Create an unique id to tag training job and model name. \n",
    "id = int(time.time())\n",
    "\n",
    "TRAINING_JOB_NAME = f\"whisper-ga-{id}\"\n",
    "print('Training job name: ', TRAINING_JOB_NAME)\n",
    "\n",
    "hyperparameters = {'max_steps':200, # you can increase the max steps to improve model accuracy\n",
    "                   'train_batch_size': training_batch_size,\n",
    "                   'eval_batch_size': eval_batch_size,\n",
    "                   'model_name': \"openai/whisper-tiny\",\n",
    "                   'language': \"en\",\n",
    "                   'dataloader_num_workers': 8,\n",
    "                  }\n",
    "\n",
    "# Define metrics definitions, such metrics will be extracted from training script's printed logs and send to cloudwatch\n",
    "metric_definitions=[\n",
    "        {'Name': 'eval_loss', 'Regex': \"'eval_loss': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'eval_wer', 'Regex': \"'eval_wer': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'eval_runtime', 'Regex': \"'eval_runtime': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'eval_samples_per_second', 'Regex': \"'eval_samples_per_second': ([0-9]+(.|e\\-)[0-9]+),?\"},\n",
    "        {'Name': 'epoch', 'Regex': \"'epoch': ([0-9]+(.|e\\-)[0-9]+),?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbeff9ac-fdf0-4ce7-9315-c93d93809eb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Point the training data to the s3 uri. Use FastFile to \"mount\" the s3 files directly instead of copying to local disk\n",
    "from sagemaker.inputs import TrainingInput\n",
    "training_input_path=s3uri\n",
    "\n",
    "training = TrainingInput(\n",
    "    s3_data_type='S3Prefix', # Available Options: S3Prefix | ManifestFile | AugmentedManifestFile\n",
    "    s3_data=training_input_path,\n",
    "    distribution='FullyReplicated', # Available Options: FullyReplicated | ShardedByS3Key \n",
    "    input_mode='FastFile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41852f30-0d50-41bc-8840-d121ca90d95b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: whisper-ga-1700694190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-22 23:03:11 Starting - Starting the training job......\n",
      "2023-11-22 23:03:55 Starting - Preparing the instances for training......\n",
      "2023-11-22 23:04:58 Downloading - Downloading input data...\n",
      "2023-11-22 23:05:28 Training - Downloading the training image.................................\n",
      "2023-11-22 23:11:05 Training - Training image download completed. Training in progress...\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:24,627 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:24,645 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:24,655 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:24,657 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:26,434 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.25.1 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.8/5.8 MB 69.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=2.6.1 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: evaluate>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.26.132)\u001b[0m\n",
      "\u001b[34mCollecting jiwer (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading jiwer-3.0.3-py3-none-any.whl (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.10.0.post2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (3.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (2023.5.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (2.28.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (0.13.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (12.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (2023.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (3.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.6.1->-r requirements.txt (line 2)) (0.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.132 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 4)) (1.29.132)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 4)) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 4)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[34mCollecting rapidfuzz<4,>=3 (from jiwer->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 83.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (1.10.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.14 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (5.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (0.56.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soundfile>=0.12.1 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pooch<1.7,>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (1.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (0.3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (4.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: lazy-loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 6)) (1.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.132->boto3->-r requirements.txt (line 4)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.132->boto3->-r requirements.txt (line 4)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (22.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (6.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (4.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (1.9.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (1.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->-r requirements.txt (line 2)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 6)) (0.39.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 6)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: appdirs>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from pooch<1.7,>=1.0->librosa->-r requirements.txt (line 6)) (1.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.25.1->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.25.1->-r requirements.txt (line 1)) (2023.5.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa->-r requirements.txt (line 6)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 6)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.6.1->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets>=2.6.1->-r requirements.txt (line 2)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 6)) (2.21)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.132->boto3->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: rapidfuzz, jiwer, transformers\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mSuccessfully installed jiwer-3.0.3 rapidfuzz-3.5.2 transformers-4.25.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.1.2 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,487 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,488 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,525 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,554 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,584 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,597 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"dataloader_num_workers\": 8,\n",
      "        \"eval_batch_size\": 8,\n",
      "        \"language\": \"en\",\n",
      "        \"max_steps\": 100,\n",
      "        \"model_name\": \"openai/whisper-tiny\",\n",
      "        \"train_batch_size\": 16\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p3.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"whisper-ga-1700694190\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-566765136679/whisper-ga-1700694190/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataloader_num_workers\":8,\"eval_batch_size\":8,\"language\":\"en\",\"max_steps\":100,\"model_name\":\"openai/whisper-tiny\",\"train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p3.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-566765136679/whisper-ga-1700694190/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p3.2xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"dataloader_num_workers\":8,\"eval_batch_size\":8,\"language\":\"en\",\"max_steps\":100,\"model_name\":\"openai/whisper-tiny\",\"train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"whisper-ga-1700694190\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-566765136679/whisper-ga-1700694190/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataloader_num_workers\",\"8\",\"--eval_batch_size\",\"8\",\"--language\",\"en\",\"--max_steps\",\"100\",\"--model_name\",\"openai/whisper-tiny\",\"--train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_DATALOADER_NUM_WORKERS=8\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LANGUAGE=en\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_STEPS=100\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=openai/whisper-tiny\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 train.py --dataloader_num_workers 8 --eval_batch_size 8 --language en --max_steps 100 --model_name openai/whisper-tiny --train_batch_size 16\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:37,637 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m2023-11-22 23:11:43,330 - __main__ - INFO - Data loaded\u001b[0m\n",
      "\u001b[34mDownloading (…)rocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)rocessor_config.json: 100%|██████████| 185k/185k [00:00<00:00, 83.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer_config.json: 100%|██████████| 805/805 [00:00<00:00, 7.26MB/s]\u001b[0m\n",
      "\u001b[34mDownloading vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading vocab.json: 100%|██████████| 836k/836k [00:00<00:00, 66.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading tokenizer.json: 100%|██████████| 2.48M/2.48M [00:00<00:00, 67.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading merges.txt: 100%|██████████| 494k/494k [00:00<00:00, 47.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading normalizer.json: 100%|██████████| 52.7k/52.7k [00:00<00:00, 34.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading added_tokens.json: 100%|██████████| 34.6k/34.6k [00:00<00:00, 161MB/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json:   0%|          | 0.00/2.08k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading (…)cial_tokens_map.json: 100%|██████████| 2.08k/2.08k [00:00<00:00, 16.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading config.json: 100%|██████████| 1.97k/1.97k [00:00<00:00, 16.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   2%|▏         | 21.0M/967M [00:00<00:15, 62.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   3%|▎         | 31.5M/967M [00:00<00:15, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   4%|▍         | 41.9M/967M [00:00<00:22, 41.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   7%|▋         | 62.9M/967M [00:01<00:16, 55.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:   8%|▊         | 73.4M/967M [00:01<00:19, 45.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  10%|▉         | 94.4M/967M [00:01<00:14, 58.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  12%|█▏        | 115M/967M [00:01<00:12, 68.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  13%|█▎        | 126M/967M [00:02<00:13, 62.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  14%|█▍        | 136M/967M [00:02<00:12, 65.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  15%|█▌        | 147M/967M [00:02<00:12, 63.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  17%|█▋        | 168M/967M [00:02<00:11, 71.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  18%|█▊        | 178M/967M [00:02<00:12, 62.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  20%|█▉        | 189M/967M [00:03<00:13, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  21%|██        | 199M/967M [00:03<00:16, 45.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  23%|██▎       | 220M/967M [00:03<00:13, 54.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  25%|██▍       | 241M/967M [00:04<00:13, 54.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  26%|██▌       | 252M/967M [00:04<00:14, 50.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  27%|██▋       | 262M/967M [00:04<00:12, 56.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  28%|██▊       | 273M/967M [00:04<00:13, 50.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  29%|██▉       | 283M/967M [00:05<00:13, 49.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  30%|███       | 294M/967M [00:05<00:14, 46.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  31%|███▏      | 304M/967M [00:05<00:14, 46.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  33%|███▎      | 315M/967M [00:05<00:16, 38.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  34%|███▎      | 325M/967M [00:06<00:15, 40.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  36%|███▌      | 346M/967M [00:06<00:13, 44.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  37%|███▋      | 357M/967M [00:06<00:13, 46.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  39%|███▉      | 377M/967M [00:07<00:12, 45.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  40%|████      | 388M/967M [00:07<00:12, 47.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  41%|████      | 398M/967M [00:07<00:12, 44.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  43%|████▎     | 419M/967M [00:08<00:11, 45.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  44%|████▍     | 430M/967M [00:08<00:11, 48.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  46%|████▌     | 440M/967M [00:08<00:09, 56.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  47%|████▋     | 451M/967M [00:08<00:11, 46.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  49%|████▉     | 472M/967M [00:09<00:09, 54.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  50%|████▉     | 482M/967M [00:09<00:09, 51.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  52%|█████▏    | 503M/967M [00:09<00:08, 54.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  54%|█████▍    | 524M/967M [00:09<00:07, 60.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  55%|█████▌    | 535M/967M [00:10<00:07, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  57%|█████▋    | 556M/967M [00:10<00:06, 59.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  60%|█████▉    | 577M/967M [00:10<00:06, 63.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  62%|██████▏   | 598M/967M [00:10<00:04, 75.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  63%|██████▎   | 608M/967M [00:11<00:04, 77.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  65%|██████▌   | 629M/967M [00:11<00:05, 65.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  67%|██████▋   | 650M/967M [00:11<00:04, 67.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  68%|██████▊   | 661M/967M [00:12<00:05, 59.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  70%|███████   | 682M/967M [00:12<00:04, 58.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  73%|███████▎  | 703M/967M [00:12<00:04, 61.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  74%|███████▎  | 713M/967M [00:13<00:04, 53.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  76%|███████▌  | 734M/967M [00:13<00:04, 51.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  78%|███████▊  | 755M/967M [00:13<00:04, 52.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  80%|████████  | 776M/967M [00:14<00:03, 62.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  81%|████████▏ | 786M/967M [00:14<00:02, 64.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  82%|████████▏ | 797M/967M [00:14<00:02, 59.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  83%|████████▎ | 807M/967M [00:14<00:03, 42.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  85%|████████▍ | 818M/967M [00:15<00:03, 44.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  86%|████████▌ | 828M/967M [00:15<00:03, 39.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  87%|████████▋ | 839M/967M [00:15<00:03, 38.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  89%|████████▉ | 860M/967M [00:16<00:02, 48.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  91%|█████████ | 881M/967M [00:16<00:01, 51.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  92%|█████████▏| 891M/967M [00:16<00:01, 51.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  93%|█████████▎| 902M/967M [00:16<00:01, 51.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  94%|█████████▍| 912M/967M [00:17<00:01, 50.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  96%|█████████▋| 933M/967M [00:17<00:00, 61.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin:  99%|█████████▊| 954M/967M [00:17<00:00, 55.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin: 100%|█████████▉| 965M/967M [00:18<00:00, 49.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading pytorch_model.bin: 100%|██████████| 967M/967M [00:18<00:00, 52.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading builder script: 100%|██████████| 4.49k/4.49k [00:00<00:00, 4.93MB/s]\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mmax_steps is given, it will override any value given in num_train_epochs\u001b[0m\n",
      "\u001b[34mUsing cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34mUsing cuda_amp half precision backend\u001b[0m\n",
      "\u001b[34m2023-11-22 23:12:07,410 - __main__ - INFO - Start training\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m0%|          | 0/100 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m1%|          | 1/100 [00:34<56:37, 34.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 2/100 [00:35<24:18, 14.88s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 3/100 [00:36<13:51,  8.57s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 4/100 [00:37<09:01,  5.64s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 5/100 [00:38<06:20,  4.01s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 6/100 [00:40<04:44,  3.02s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 7/100 [00:41<03:42,  2.40s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 8/100 [00:42<03:02,  1.99s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 9/100 [00:46<03:57,  2.61s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 10/100 [00:47<03:13,  2.15s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 11/100 [00:48<02:41,  1.81s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 12/100 [00:49<02:20,  1.60s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 13/100 [00:50<02:06,  1.45s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 14/100 [00:51<01:56,  1.35s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 15/100 [00:52<01:48,  1.28s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 16/100 [00:53<01:43,  1.23s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 17/100 [01:03<05:08,  3.72s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 18/100 [01:04<04:00,  2.93s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 19/100 [01:05<03:13,  2.39s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 20/100 [01:08<03:11,  2.39s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 21/100 [01:09<02:38,  2.01s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 22/100 [01:11<02:43,  2.09s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 23/100 [01:12<02:18,  1.80s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 24/100 [01:13<02:00,  1.59s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 25/100 [01:14<01:48,  1.45s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 26/100 [01:15<01:39,  1.35s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 27/100 [01:17<01:33,  1.27s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 28/100 [01:18<01:28,  1.22s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 29/100 [01:19<01:24,  1.19s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 30/100 [01:20<01:21,  1.17s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 31/100 [01:21<01:19,  1.15s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 32/100 [01:22<01:17,  1.14s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 33/100 [01:23<01:15,  1.13s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 34/100 [01:24<01:00,  1.09it/s]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 35/100 [01:58<11:48, 10.90s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 36/100 [01:59<08:29,  7.96s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 37/100 [02:00<06:12,  5.91s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 38/100 [02:01<04:37,  4.47s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 39/100 [02:02<03:31,  3.46s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 40/100 [02:03<02:45,  2.76s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 41/100 [02:04<02:13,  2.26s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 42/100 [02:06<01:51,  1.92s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 43/100 [02:11<02:49,  2.97s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 44/100 [02:12<02:15,  2.42s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 45/100 [02:13<01:51,  2.02s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 46/100 [02:14<01:34,  1.75s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 47/100 [02:15<01:22,  1.56s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 48/100 [02:17<01:14,  1.42s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 49/100 [02:18<01:07,  1.33s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 50/100 [02:19<01:03,  1.27s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 51/100 [02:20<00:59,  1.22s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 52/100 [02:21<00:56,  1.19s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 53/100 [02:22<00:54,  1.16s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 54/100 [02:23<00:52,  1.15s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 55/100 [02:33<02:41,  3.59s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 56/100 [02:34<02:09,  2.93s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 57/100 [02:35<01:42,  2.39s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 58/100 [02:36<01:24,  2.01s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 59/100 [02:37<01:11,  1.75s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 60/100 [02:38<01:02,  1.56s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 61/100 [02:40<00:55,  1.43s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 62/100 [02:41<00:50,  1.33s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 63/100 [02:42<00:46,  1.26s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 64/100 [02:43<00:43,  1.22s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 65/100 [02:44<00:41,  1.19s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 66/100 [02:45<00:39,  1.16s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 67/100 [02:46<00:37,  1.15s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 68/100 [02:47<00:29,  1.08it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 69/100 [03:21<05:39, 10.95s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 70/100 [03:22<03:59,  8.00s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 71/100 [03:23<02:52,  5.94s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 72/100 [03:24<02:05,  4.49s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 73/100 [03:25<01:33,  3.48s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 74/100 [03:27<01:11,  2.77s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 75/100 [03:28<00:56,  2.27s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 76/100 [03:29<00:46,  1.92s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 77/100 [03:30<00:38,  1.68s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 78/100 [03:34<00:53,  2.45s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 79/100 [03:35<00:42,  2.05s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 80/100 [03:36<00:35,  1.76s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 81/100 [03:37<00:29,  1.57s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 82/100 [03:39<00:25,  1.43s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 83/100 [03:40<00:22,  1.34s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 84/100 [03:41<00:20,  1.27s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 85/100 [03:52<01:03,  4.21s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 86/100 [03:53<00:45,  3.28s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 87/100 [03:54<00:34,  2.63s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 88/100 [03:55<00:26,  2.18s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 89/100 [03:56<00:20,  1.86s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 90/100 [03:57<00:16,  1.64s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 91/100 [03:59<00:13,  1.48s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 92/100 [04:00<00:10,  1.37s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 93/100 [04:01<00:09,  1.29s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 94/100 [04:02<00:07,  1.24s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 95/100 [04:03<00:05,  1.20s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 96/100 [04:04<00:04,  1.17s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 97/100 [04:05<00:03,  1.16s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 98/100 [04:06<00:02,  1.14s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 99/100 [04:07<00:01,  1.14s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [04:09<00:00,  1.13s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 249.3111, 'train_samples_per_second': 6.418, 'train_steps_per_second': 0.401, 'train_loss': 2.9237460327148437, 'epoch': 2.94}\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [04:09<00:00,  1.13s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 100/100 [04:09<00:00,  2.49s/it]\u001b[0m\n",
      "\u001b[34m2023-11-22 23:16:16,736 - __main__ - INFO - Start evaluating\u001b[0m\n",
      "\u001b[34m0%|          | 0/65 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/65 [00:00<00:26,  2.38it/s]\u001b[0m\n",
      "\u001b[34m5%|▍         | 3/65 [00:01<00:41,  1.49it/s]\u001b[0m\n",
      "\u001b[34m6%|▌         | 4/65 [00:02<00:47,  1.30it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 5/65 [00:03<00:49,  1.20it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 6/65 [00:04<00:49,  1.19it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 7/65 [00:05<00:51,  1.12it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 8/65 [00:06<00:53,  1.07it/s]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 9/65 [00:07<00:55,  1.01it/s]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 10/65 [00:08<00:49,  1.10it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 11/65 [00:09<00:53,  1.02it/s]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 12/65 [00:10<00:55,  1.06s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 13/65 [00:12<00:57,  1.11s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 14/65 [00:13<00:56,  1.10s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 15/65 [00:14<00:54,  1.08s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 16/65 [00:15<00:54,  1.12s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 17/65 [00:16<00:53,  1.11s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 18/65 [00:17<00:53,  1.13s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 19/65 [00:18<00:51,  1.12s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 20/65 [00:19<00:50,  1.13s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 21/65 [00:20<00:47,  1.08s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 22/65 [00:21<00:45,  1.06s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 23/65 [00:22<00:43,  1.03s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 24/65 [00:23<00:43,  1.05s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 25/65 [00:24<00:40,  1.02s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 26/65 [00:25<00:38,  1.02it/s]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 27/65 [00:26<00:39,  1.03s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 28/65 [00:28<00:39,  1.08s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 29/65 [00:29<00:37,  1.05s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 30/65 [00:30<00:37,  1.07s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 31/65 [00:31<00:39,  1.15s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 32/65 [00:32<00:38,  1.16s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 33/65 [00:33<00:36,  1.14s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 34/65 [00:34<00:32,  1.05s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 35/65 [00:35<00:29,  1.02it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 36/65 [00:36<00:29,  1.02s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 37/65 [00:37<00:28,  1.03s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 38/65 [00:38<00:29,  1.08s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 39/65 [00:39<00:27,  1.07s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 40/65 [00:40<00:26,  1.07s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 41/65 [00:42<00:26,  1.09s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 42/65 [00:43<00:25,  1.09s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 43/65 [00:44<00:24,  1.13s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 44/65 [00:45<00:21,  1.04s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 45/65 [00:46<00:20,  1.04s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 46/65 [00:47<00:19,  1.05s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 47/65 [00:48<00:18,  1.05s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 48/65 [00:49<00:18,  1.06s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 49/65 [00:50<00:16,  1.01s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 50/65 [00:51<00:14,  1.01it/s]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 51/65 [00:52<00:14,  1.06s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 52/65 [00:53<00:13,  1.06s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 53/65 [00:54<00:12,  1.01s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 54/65 [00:55<00:11,  1.03s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 55/65 [00:56<00:10,  1.04s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 56/65 [00:57<00:08,  1.01it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 57/65 [00:58<00:08,  1.05s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 58/65 [00:59<00:07,  1.06s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 59/65 [01:00<00:06,  1.06s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 60/65 [01:01<00:05,  1.05s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 61/65 [01:02<00:04,  1.04s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 62/65 [01:03<00:02,  1.04it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 63/65 [01:04<00:02,  1.03s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 64/65 [01:05<00:01,  1.05s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 65/65 [01:06<00:00,  1.16it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 65/65 [01:22<00:00,  1.26s/it]\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m2023-11-22 23:17:56,946 - __main__ - INFO - Start saving\u001b[0m\n",
      "\u001b[34m2023-11-22 23:17:58,386 - __main__ - INFO - All done\u001b[0m\n",
      "\u001b[34m2023-11-22 23:17:59,344 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-11-22 23:17:59,344 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-11-22 23:17:59,344 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-11-22 23:18:07 Uploading - Uploading generated training model\n",
      "2023-11-22 23:18:34 Completed - Training job completed\n",
      "Training seconds: 815\n",
      "Billable seconds: 815\n"
     ]
    }
   ],
   "source": [
    "# Create the HuggingFace Estimator and kick off the training with \"fit\". Note that as of the writing, the latest hugging face training image has version of transformers_version='4.17.0' and pytorch_version='1.10.2', the transformer version can be upgraded in the requirements.txt.\n",
    "# More details on training images, see https://github.com/aws/deep-learning-containers/blob/master/available_images.md\n",
    "OUTPUT_PATH= f's3://{BUCKET}/{PREFIX}/{TRAINING_JOB_NAME}/output/'\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                                    source_dir='./scripts',\n",
    "                                    output_path= OUTPUT_PATH, \n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=1,\n",
    "                                    transformers_version='4.28.1',\n",
    "                                    pytorch_version='2.0.0',\n",
    "                                    py_version='py310',\n",
    "                                    role=ROLE,\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    metric_definitions = metric_definitions,\n",
    "                                    volume_size=200,\n",
    "                                    distribution=distribution,\n",
    "                                   )\n",
    "\n",
    "#Starts the training job using the fit function, training takes approximately 2 hours to complete.\n",
    "huggingface_estimator.fit({'train': training}, job_name=TRAINING_JOB_NAME)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
